{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac3978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193bb601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from the website.\n"
     ]
    }
   ],
   "source": [
    "# Specify the URL of the website you want to scrape\n",
    "url = 'https://finance.yahoo.com/quote/TATAPOWER.NS'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find and extract the desired data from the HTML\n",
    "    # You can use various methods provided by BeautifulSoup to navigate and search the HTML structure\n",
    "    \n",
    "    # Example: Extract all the text from paragraph elements\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for paragraph in paragraphs:\n",
    "        print(paragraph.text)\n",
    "else:\n",
    "    print('Failed to retrieve data from the website.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61fa9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from the website.\n"
     ]
    }
   ],
   "source": [
    "# Specify the URL of the website you want to scrape\n",
    "url = 'https://finance.yahoo.com/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find and extract the desired data from the HTML\n",
    "    # You can use various methods provided by BeautifulSoup to navigate and search the HTML structure\n",
    "    \n",
    "    # Example: Extract data from a table and save it in a CSV file\n",
    "    table = soup.find('table')  # Assuming the data is in a table\n",
    "    rows = table.find_all('tr')  # Find all rows in the table\n",
    "    \n",
    "    # Create a CSV file and write the data to it\n",
    "    with open('data.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write the header row if needed\n",
    "        # header = [header1, header2, ...]\n",
    "        # writer.writerow(header)\n",
    "        \n",
    "        # Write each row of data to the CSV file\n",
    "        for row in rows:\n",
    "            # Extract the data from each cell in the row\n",
    "            cells = row.find_all('td')\n",
    "            data = [cell.text.strip() for cell in cells]\n",
    "            \n",
    "            # Write the data to the CSV file\n",
    "            writer.writerow(data)\n",
    "    \n",
    "    # Alternatively, you can create a DataFrame from the extracted data\n",
    "    # and perform further operations on it using pandas\n",
    "    df = pd.read_csv('data.csv')\n",
    "    print(df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "else:\n",
    "    print('Failed to retrieve data from the website.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d02057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a245dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bc801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6e1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd4142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e233c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed7357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6797fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2d964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "driver = webdriver.Chrome()\n",
    "url = 'https://finance.yahoo.com/quote/TATAPOWER.NS/history?period1=820454400&period2=1682899200&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to the bottom of the page to load more data\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Keep scrolling until the desired number of rows is reached\n",
    "while len(driver.find_elements('xpath', '//table/tbody/tr')) < 400:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# Get the page source after scrolling\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the Selenium webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a BeautifulSoup object with the page source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find the table containing the historical data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Create empty lists to store the data\n",
    "dates = []\n",
    "open_prices = []\n",
    "high_prices = []\n",
    "low_prices = []\n",
    "close_prices = []\n",
    "adj_close_prices = []\n",
    "volumes = []\n",
    "\n",
    "# Extract data from each row of the table\n",
    "for row in table.tbody.find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) == 7:\n",
    "        date = cols[0].text\n",
    "        open_price = float(cols[1].text.replace(',', ''))\n",
    "        high_price = float(cols[2].text.replace(',', ''))\n",
    "        low_price = float(cols[3].text.replace(',', ''))\n",
    "        close_price = float(cols[4].text.replace(',', ''))\n",
    "        adj_close_price = float(cols[5].text.replace(',', ''))\n",
    "        volume = int(cols[6].text.replace(',', ''))\n",
    "        \n",
    "        dates.append(date)\n",
    "        open_prices.append(open_price)\n",
    "        high_prices.append(high_price)\n",
    "        low_prices.append(low_price)\n",
    "        close_prices.append(close_price)\n",
    "        adj_close_prices.append(adj_close_price)\n",
    "        volumes.append(volume)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': open_prices,\n",
    "    'High': high_prices,\n",
    "    'Low': low_prices,\n",
    "    'Close': close_prices,\n",
    "    'Adj Close': adj_close_prices,\n",
    "    'Volume': volumes\n",
    "})\n",
    "\n",
    "# Define the file path for the CSV file\n",
    "csv_file_path = 'stock_data.csv'\n",
    "\n",
    "# Check if the CSV file exists and delete it if it does\n",
    "if os.path.exists(csv_file_path):\n",
    "    os.remove(csv_file_path)\n",
    "\n",
    "# Save the data as a CSV file\n",
    "data.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"Data saved as CSV:\", csv_file_path)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb66c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
